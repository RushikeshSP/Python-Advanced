{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression\n",
    "Logistic Regression\n",
    "Decision Tree\n",
    "SVM\n",
    "Naive Bayes\n",
    "kNN\n",
    "K-Means\n",
    "Random Forest\n",
    "Dimensionality Reduction Algorithms\n",
    "Gradient Boosting algorithms\n",
    "GBM\n",
    "XGBoost\n",
    "LightGBM\n",
    "CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Linear Regression\n",
    "It is used to estimate real values (cost of houses, number of calls, total sales etc.) based on continuous variable(s). Here, we establish relationship between independent and dependent variables by fitting a best line. This best fit line is known as regression line and represented by a linear equation Y= a *X + b.\n",
    "\n",
    "The best way to understand linear regression is to relive this experience of childhood. Let us say, you ask a child in fifth grade to arrange people in his class by increasing order of weight, without asking them their weights! What do you think the child will do? He / she would likely look (visually analyze) at the height and build of people and arrange them using a combination of these visible parameters. This is linear regression in real life! The child has actually figured out that height and build would be correlated to the weight by a relationship, which looks like the equation above.\n",
    "\n",
    "In this equation:\n",
    "\n",
    "Y – Dependent Variable\n",
    "a – Slope\n",
    "X – Independent variable\n",
    "b – Intercept\n",
    "These coefficients a and b are derived based on minimizing the sum of squared difference of distance between data points and regression line.\n",
    "\n",
    "Look at the below example. Here we have identified the best fit line having linear equation y=0.2811x+13.9. Now using this equation, we can find the weight, knowing the height of a person.\n",
    "Linear Regression is mainly of two types: Simple Linear Regression and Multiple Linear Regression. Simple Linear Regression is characterized by one independent variable. And, Multiple Linear Regression(as the name suggests) is characterized by multiple (more than 1) independent variables. While finding the best fit line, you can fit a polynomial or curvilinear regression. And these are known as polynomial or curvilinear regression.\n",
    "\n",
    "Here’s a coding window to try out your hand and build your own linear regression model in Python:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# read the train and test dataset\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "print(train_data.head())\n",
    "\n",
    "# shape of the dataset\n",
    "print('\\nShape of training data :',train_data.shape)\n",
    "print('\\nShape of testing data :',test_data.shape)\n",
    "\n",
    "# Now, we need to predict the missing target variable in the test data\n",
    "# target variable - Item_Outlet_Sales\n",
    "\n",
    "# seperate the independent and target variable on training data\n",
    "train_x = train_data.drop(columns=['Item_Outlet_Sales'],axis=1)\n",
    "train_y = train_data['Item_Outlet_Sales']\n",
    "\n",
    "# seperate the independent and target variable on training data\n",
    "test_x = test_data.drop(columns=['Item_Outlet_Sales'],axis=1)\n",
    "test_y = test_data['Item_Outlet_Sales']\n",
    "\n",
    "'''\n",
    "Create the object of the Linear Regression model\n",
    "You can also add other parameters and test your code here\n",
    "Some parameters are : fit_intercept and normalize\n",
    "Documentation of sklearn LinearRegression: \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "\n",
    " '''\n",
    "model = LinearRegression()\n",
    "\n",
    "# fit the model with the training data\n",
    "model.fit(train_x,train_y)\n",
    "\n",
    "# coefficeints of the trained model\n",
    "print('\\nCoefficient of model :', model.coef_)\n",
    "\n",
    "# intercept of the model\n",
    "print('\\nIntercept of model',model.intercept_)\n",
    "\n",
    "predict_train = model.predict(train_x)\n",
    "print('\\nItem_Outlet_Sales on training data',predict_train) \n",
    "\n",
    "# Root Mean Squared Error on training dataset\n",
    "rmse_train = mean_squared_error(train_y,predict_train)**(0.5)\n",
    "print('\\nRMSE on train dataset : ', rmse_train)\n",
    "\n",
    "# predict the target on the testing dataset\n",
    "predict_test = model.predict(test_x)\n",
    "print('\\nItem_Outlet_Sales on test data',predict_test) \n",
    "\n",
    "# Root Mean Squared Error on testing dataset\n",
    "rmse_test = mean_squared_error(test_y,predict_test)**(0.5)\n",
    "print('\\nRMSE on test dataset : ', rmse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Logistic Regression\n",
    "Don’t get confused by its name! It is a classification not a regression algorithm. It is used to estimate discrete values ( Binary values like 0/1, yes/no, true/false ) based on given set of independent variable(s). In simple words, it predicts the probability of occurrence of an event by fitting data to a logit function. Hence, it is also known as logit regression. Since, it predicts the probability, its output values lies between 0 and 1 (as expected).\n",
    "\n",
    "Again, let us try and understand this through a simple example.\n",
    "\n",
    "Let’s say your friend gives you a puzzle to solve. There are only 2 outcome scenarios – either you solve it or you don’t. Now imagine, that you are being given wide range of puzzles / quizzes in an attempt to understand which subjects you are good at. The outcome to this study would be something like this – if you are given a trignometry based tenth grade problem, you are 70% likely to solve it. On the other hand, if it is grade fifth history question, the probability of getting an answer is only 30%. This is what Logistic Regression provides you.\n",
    "\n",
    "Coming to the math, the log odds of the outcome is modeled as a linear combination of the predictor variables.\n",
    "\n",
    "odds= p/ (1-p) = probability of event occurrence / probability of not event occurrence\n",
    "ln(odds) = ln(p/(1-p))\n",
    "logit(p) = ln(p/(1-p)) = b0+b1X1+b2X2+b3X3....+bkXk\n",
    "Above, p is the probability of presence of the characteristic of interest. It chooses parameters that maximize the likelihood of observing the sample values rather than that minimize the sum of squared errors (like in ordinary regression).\n",
    "\n",
    "Now, you may ask, why take a log? For the sake of simplicity, let’s just say that this is one of the best mathematical way to replicate a step function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# read the train and test dataset\n",
    "train_data = pd.read_csv('train-data.csv')\n",
    "test_data = pd.read_csv('test-data.csv')\n",
    "\n",
    "\n",
    "print(train_data.head())\n",
    "\n",
    "# shape of the dataset\n",
    "print('Shape of training data :',train_data.shape)\n",
    "print('Shape of testing data :',test_data.shape)\n",
    "\n",
    "# Now, we need to predict the missing target variable in the test data\n",
    "# target variable - Survived\n",
    "\n",
    "# seperate the independent and target variable on training data\n",
    "train_x = train_data.drop(columns=['Survived'],axis=1)\n",
    "train_y = train_data['Survived']\n",
    "\n",
    "# seperate the independent and target variable on testing data\n",
    "test_x = test_data.drop(columns=['Survived'],axis=1)\n",
    "test_y = test_data['Survived']\n",
    "\n",
    "'''\n",
    "Create the object of the Logistic Regression model\n",
    "You can also add other parameters and test your code here\n",
    "Some parameters are : fit_intercept and penalty\n",
    "Documentation of sklearn LogisticRegression: \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    " '''\n",
    "model = LogisticRegression()\n",
    "\n",
    "# fit the model with the training data\n",
    "model.fit(train_x,train_y)\n",
    "\n",
    "# coefficeints of the trained model\n",
    "print('Coefficient of model :', model.coef_)\n",
    "\n",
    "# intercept of the model\n",
    "print('Intercept of model',model.intercept_)\n",
    "\n",
    "# predict the target on the train dataset\n",
    "predict_train = model.predict(train_x)\n",
    "print('Target on train data',predict_train) \n",
    "\n",
    "# Accuray Score on train dataset\n",
    "accuracy_train = accuracy_score(train_y,predict_train)\n",
    "print('accuracy_score on train dataset : ', accuracy_train)\n",
    "\n",
    "# predict the target on the test dataset\n",
    "predict_test = model.predict(test_x)\n",
    "print('Target on test data',predict_test) \n",
    "\n",
    "# Accuracy Score on test dataset\n",
    "accuracy_test = accuracy_score(test_y,predict_test)\n",
    "print('accuracy_score on test dataset : ', accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Decision Tree\n",
    "This is one of my favorite algorithm and I use it quite frequently. It is a type of supervised learning algorithm that is mostly used for classification problems. Surprisingly, it works for both categorical and continuous dependent variables. In this algorithm, we split the population into two or more homogeneous sets. This is done based on most significant attributes/ independent variables to make as distinct groups as possible. For more details, you can read: Decision Tree Simplified.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# read the train and test dataset\n",
    "train_data = pd.read_csv('train-data.csv')\n",
    "test_data = pd.read_csv('test-data.csv')\n",
    "\n",
    "# shape of the dataset\n",
    "print('Shape of training data :',train_data.shape)\n",
    "print('Shape of testing data :',test_data.shape)\n",
    "\n",
    "# Now, we need to predict the missing target variable in the test data\n",
    "# target variable - Survived\n",
    "\n",
    "# seperate the independent and target variable on training data\n",
    "train_x = train_data.drop(columns=['Survived'],axis=1)\n",
    "train_y = train_data['Survived']\n",
    "\n",
    "# seperate the independent and target variable on testing data\n",
    "test_x = test_data.drop(columns=['Survived'],axis=1)\n",
    "test_y = test_data['Survived']\n",
    "\n",
    "'''\n",
    "Create the object of the Decision Tree model\n",
    "You can also add other parameters and test your code here\n",
    "Some parameters are : max_depth and max_features\n",
    "Documentation of sklearn DecisionTreeClassifier: \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "\n",
    " '''\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# fit the model with the training data\n",
    "model.fit(train_x,train_y)\n",
    "\n",
    "# depth of the decision tree\n",
    "print('Depth of the Decision Tree :', model.get_depth())\n",
    "\n",
    "# predict the target on the train dataset\n",
    "predict_train = model.predict(train_x)\n",
    "print('Target on train data',predict_train) \n",
    "\n",
    "# Accuray Score on train dataset\n",
    "accuracy_train = accuracy_score(train_y,predict_train)\n",
    "print('accuracy_score on train dataset : ', accuracy_train)\n",
    "\n",
    "# predict the target on the test dataset\n",
    "predict_test = model.predict(test_x)\n",
    "print('Target on test data',predict_test) \n",
    "\n",
    "# Accuracy Score on test dataset\n",
    "accuracy_test = accuracy_score(test_y,predict_test)\n",
    "print('accuracy_score on test dataset : ', accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. SVM (Support Vector Machine)\n",
    "It is a classification method. In this algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate.\n",
    "\n",
    "For example, if we only had two features like Height and Hair length of an individual, we’d first plot these two variables in two dimensional space where each point has two co-ordinates (these co-ordinates are known as Support Vectors)\n",
    "\n",
    "SVM1\n",
    "\n",
    "Now, we will find some line that splits the data between the two differently classified groups of data. This will be the line such that the distances from the closest point in each of the two groups will be farthest away.\n",
    "\n",
    "SVM2\n",
    "\n",
    "In the example shown above, the line which splits the data into two differently classified groups is the black line, since the two closest points are the farthest apart from the line. This line is our classifier. Then, depending on where the testing data lands on either side of the line, that’s what class we can classify the new data as.\n",
    "\n",
    "More: Simplified Version of Support Vector Machine\n",
    "\n",
    "Think of this algorithm as playing JezzBall in n-dimensional space. The tweaks in the game are:\n",
    "\n",
    "You can draw lines/planes at any angles (rather than just horizontal or vertical as in the classic game)\n",
    "The objective of the game is to segregate balls of different colors in different rooms.\n",
    "And the balls are not moving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# read the train and test dataset\n",
    "train_data = pd.read_csv('train-data.csv')\n",
    "test_data = pd.read_csv('test-data.csv')\n",
    "\n",
    "# shape of the dataset\n",
    "print('Shape of training data :',train_data.shape)\n",
    "print('Shape of testing data :',test_data.shape)\n",
    "\n",
    "# Now, we need to predict the missing target variable in the test data\n",
    "# target variable - Survived\n",
    "\n",
    "# seperate the independent and target variable on training data\n",
    "train_x = train_data.drop(columns=['Survived'],axis=1)\n",
    "train_y = train_data['Survived']\n",
    "\n",
    "# seperate the independent and target variable on testing data\n",
    "test_x = test_data.drop(columns=['Survived'],axis=1)\n",
    "test_y = test_data['Survived']\n",
    "\n",
    "'''\n",
    "Create the object of the Support Vector Classifier model\n",
    "You can also add other parameters and test your code here\n",
    "Some parameters are : kernal and degree\n",
    "Documentation of sklearn Support Vector Classifier: \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "\n",
    " '''\n",
    "model = SVC()\n",
    "\n",
    "# fit the model with the training data\n",
    "model.fit(train_x,train_y)\n",
    "\n",
    "# predict the target on the train dataset\n",
    "predict_train = model.predict(train_x)\n",
    "print('Target on train data',predict_train) \n",
    "\n",
    "# Accuray Score on train dataset\n",
    "accuracy_train = accuracy_score(train_y,predict_train)\n",
    "print('accuracy_score on train dataset : ', accuracy_train)\n",
    "\n",
    "# predict the target on the test dataset\n",
    "predict_test = model.predict(test_x)\n",
    "print('Target on test data',predict_test) \n",
    "\n",
    "# Accuracy Score on test dataset\n",
    "accuracy_test = accuracy_score(test_y,predict_test)\n",
    "print('accuracy_score on test dataset : ', accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Naive Bayes\n",
    "It is a classification technique based on Bayes’ theorem with an assumption of independence between predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. For example, a fruit may be considered to be an apple if it is red, round, and about 3 inches in diameter. Even if these features depend on each other or upon the existence of the other features, a naive Bayes classifier would consider all of these properties to independently contribute to the probability that this fruit is an apple.\n",
    "\n",
    "Naive Bayesian model is easy to build and particularly useful for very large data sets. Along with simplicity, Naive Bayes is known to outperform even highly sophisticated classification methods.\n",
    "\n",
    "Bayes theorem provides a way of calculating posterior probability P(c|x) from P(c), P(x) and P(x|c). Look at the equation below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# read the train and test dataset\n",
    "train_data = pd.read_csv('train-data.csv')\n",
    "test_data = pd.read_csv('test-data.csv')\n",
    "\n",
    "# shape of the dataset\n",
    "print('Shape of training data :',train_data.shape)\n",
    "print('Shape of testing data :',test_data.shape)\n",
    "\n",
    "# Now, we need to predict the missing target variable in the test data\n",
    "# target variable - Survived\n",
    "\n",
    "# seperate the independent and target variable on training data\n",
    "train_x = train_data.drop(columns=['Survived'],axis=1)\n",
    "train_y = train_data['Survived']\n",
    "\n",
    "# seperate the independent and target variable on testing data\n",
    "test_x = test_data.drop(columns=['Survived'],axis=1)\n",
    "test_y = test_data['Survived']\n",
    "\n",
    "'''\n",
    "Create the object of the Naive Bayes model\n",
    "You can also add other parameters and test your code here\n",
    "Some parameters are : var_smoothing\n",
    "Documentation of sklearn GaussianNB: \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html\n",
    "\n",
    " '''\n",
    "model = GaussianNB()\n",
    "\n",
    "# fit the model with the training data\n",
    "model.fit(train_x,train_y)\n",
    "\n",
    "# predict the target on the train dataset\n",
    "predict_train = model.predict(train_x)\n",
    "print('Target on train data',predict_train) \n",
    "\n",
    "# Accuray Score on train dataset\n",
    "accuracy_train = accuracy_score(train_y,predict_train)\n",
    "print('accuracy_score on train dataset : ', accuracy_train)\n",
    "\n",
    "# predict the target on the test dataset\n",
    "predict_test = model.predict(test_x)\n",
    "print('Target on test data',predict_test) \n",
    "\n",
    "# Accuracy Score on test dataset\n",
    "accuracy_test = accuracy_score(test_y,predict_test)\n",
    "print('accuracy_score on test dataset : ', accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. kNN (k- Nearest Neighbors)\n",
    "It can be used for both classification and regression problems. However, it is more widely used in classification problems in the industry. K nearest neighbors is a simple algorithm that stores all available cases and classifies new cases by a majority vote of its k neighbors. The case being assigned to the class is most common amongst its K nearest neighbors measured by a distance function.\n",
    "\n",
    "These distance functions can be Euclidean, Manhattan, Minkowski and Hamming distance. First three functions are used for continuous function and fourth one (Hamming) for categorical variables. If K = 1, then the case is simply assigned to the class of its nearest neighbor. At times, choosing K turns out to be a challenge while performing kNN modeling.\n",
    "KNN can easily be mapped to our real lives. If you want to learn about a person, of whom you have no information, you might like to find out about his close friends and the circles he moves in and gain access to his/her information!\n",
    "\n",
    "Things to consider before selecting kNN:\n",
    "\n",
    "KNN is computationally expensive\n",
    "Variables should be normalized else higher range variables can bias it\n",
    "Works on pre-processing stage more before going for kNN like an outlier, noise removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# read the train and test dataset\n",
    "train_data = pd.read_csv('train-data.csv')\n",
    "test_data = pd.read_csv('test-data.csv')\n",
    "\n",
    "# shape of the dataset\n",
    "print('Shape of training data :',train_data.shape)\n",
    "print('Shape of testing data :',test_data.shape)\n",
    "\n",
    "# Now, we need to predict the missing target variable in the test data\n",
    "# target variable - Survived\n",
    "\n",
    "# seperate the independent and target variable on training data\n",
    "train_x = train_data.drop(columns=['Survived'],axis=1)\n",
    "train_y = train_data['Survived']\n",
    "\n",
    "# seperate the independent and target variable on testing data\n",
    "test_x = test_data.drop(columns=['Survived'],axis=1)\n",
    "test_y = test_data['Survived']\n",
    "\n",
    "'''\n",
    "Create the object of the K-Nearest Neighbor model\n",
    "You can also add other parameters and test your code here\n",
    "Some parameters are : n_neighbors, leaf_size\n",
    "Documentation of sklearn K-Neighbors Classifier: \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "\n",
    " '''\n",
    "model = KNeighborsClassifier()  \n",
    "\n",
    "# fit the model with the training data\n",
    "model.fit(train_x,train_y)\n",
    "\n",
    "# Number of Neighbors used to predict the target\n",
    "print('\\nThe number of neighbors used to predict the target : ',model.n_neighbors)\n",
    "\n",
    "# predict the target on the train dataset\n",
    "predict_train = model.predict(train_x)\n",
    "print('\\nTarget on train data',predict_train) \n",
    "\n",
    "# Accuray Score on train dataset\n",
    "accuracy_train = accuracy_score(train_y,predict_train)\n",
    "print('accuracy_score on train dataset : ', accuracy_train)\n",
    "\n",
    "# predict the target on the test dataset\n",
    "predict_test = model.predict(test_x)\n",
    "print('Target on test data',predict_test) \n",
    "\n",
    "# Accuracy Score on test dataset\n",
    "accuracy_test = accuracy_score(test_y,predict_test)\n",
    "print('accuracy_score on test dataset : ', accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. K-Means\n",
    "It is a type of unsupervised algorithm which solves the clustering problem. Its procedure follows a simple and easy way to classify a given data set through a certain number of clusters (assume k clusters). Data points inside a cluster are homogeneous and heterogeneous to peer groups.\n",
    "\n",
    "Remember figuring out shapes from ink blots? k means is somewhat similar this activity. You look at the shape and spread to decipher how many different clusters / population are present!\n",
    "\n",
    "splatter_ink_blot_texture_by_maki_tak-d5p6zph\n",
    "\n",
    "How K-means forms cluster:\n",
    "\n",
    "K-means picks k number of points for each cluster known as centroids.\n",
    "Each data point forms a cluster with the closest centroids i.e. k clusters.\n",
    "Finds the centroid of each cluster based on existing cluster members. Here we have new centroids.\n",
    "As we have new centroids, repeat step 2 and 3. Find the closest distance for each data point from new centroids and get associated with new k-clusters. Repeat this process until convergence occurs i.e. centroids does not change.\n",
    "\n",
    "How to determine value of K:\n",
    "\n",
    "In K-means, we have clusters and each cluster has its own centroid. Sum of square of difference between centroid and the data points within a cluster constitutes within sum of square value for that cluster. Also, when the sum of square values for all the clusters are added, it becomes total within sum of square value for the cluster solution.\n",
    "\n",
    "We know that as the number of cluster increases, this value keeps on decreasing but if you plot the result you may see that the sum of squared distance decreases sharply up to some value of k, and then much more slowly after that. Here, we can find the optimum number of cluster.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# read the train and test dataset\n",
    "train_data = pd.read_csv('train-data.csv')\n",
    "test_data = pd.read_csv('test-data.csv')\n",
    "\n",
    "# shape of the dataset\n",
    "print('Shape of training data :',train_data.shape)\n",
    "print('Shape of testing data :',test_data.shape)\n",
    "\n",
    "# Now, we need to divide the training data into differernt clusters\n",
    "# and predict in which cluster a particular data point belongs.  \n",
    "\n",
    "'''\n",
    "Create the object of the K-Means model\n",
    "You can also add other parameters and test your code here\n",
    "Some parameters are : n_clusters and max_iter\n",
    "Documentation of sklearn KMeans: \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    " '''\n",
    "\n",
    "model = KMeans()  \n",
    "\n",
    "# fit the model with the training data\n",
    "model.fit(train_data)\n",
    "\n",
    "# Number of Clusters\n",
    "print('\\nDefault number of Clusters : ',model.n_clusters)\n",
    "\n",
    "# predict the clusters on the train dataset\n",
    "predict_train = model.predict(train_data)\n",
    "print('\\nCLusters on train data',predict_train) \n",
    "\n",
    "# predict the target on the test dataset\n",
    "predict_test = model.predict(test_data)\n",
    "print('Clusters on test data',predict_test) \n",
    "\n",
    "# Now, we will train a model with n_cluster = 3\n",
    "model_n3 = KMeans(n_clusters=3)\n",
    "\n",
    "# fit the model with the training data\n",
    "model_n3.fit(train_data)\n",
    "\n",
    "# Number of Clusters\n",
    "print('\\nNumber of Clusters : ',model_n3.n_clusters)\n",
    "\n",
    "# predict the clusters on the train dataset\n",
    "predict_train_3 = model_n3.predict(train_data)\n",
    "print('\\nCLusters on train data',predict_train_3) \n",
    "\n",
    "# predict the target on the test dataset\n",
    "predict_test_3 = model_n3.predict(test_data)\n",
    "print('Clusters on test data',predict_test_3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Random Forest\n",
    "Random Forest is a trademark term for an ensemble of decision trees. In Random Forest, we’ve collection of decision trees (so known as “Forest”). To classify a new object based on attributes, each tree gives a classification and we say the tree “votes” for that class. The forest chooses the classification having the most votes (over all the trees in the forest).\n",
    "\n",
    "Each tree is planted & grown as follows:\n",
    "\n",
    "If the number of cases in the training set is N, then sample of N cases is taken at random but with replacement. This sample will be the training set for growing the tree.\n",
    "If there are M input variables, a number m<<M is specified such that at each node, m variables are selected at random out of the M and the best split on these m is used to split the node. The value of m is held constant during the forest growing.\n",
    "Each tree is grown to the largest extent possible. There is no pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# read the train and test dataset\n",
    "train_data = pd.read_csv('train-data.csv')\n",
    "test_data = pd.read_csv('test-data.csv')\n",
    "\n",
    "# view the top 3 rows of the dataset\n",
    "print(train_data.head(3))\n",
    "\n",
    "# shape of the dataset\n",
    "print('\\nShape of training data :',train_data.shape)\n",
    "print('\\nShape of testing data :',test_data.shape)\n",
    "\n",
    "# Now, we need to predict the missing target variable in the test data\n",
    "# target variable - Survived\n",
    "\n",
    "# seperate the independent and target variable on training data\n",
    "train_x = train_data.drop(columns=['Survived'],axis=1)\n",
    "train_y = train_data['Survived']\n",
    "\n",
    "# seperate the independent and target variable on testing data\n",
    "test_x = test_data.drop(columns=['Survived'],axis=1)\n",
    "test_y = test_data['Survived']\n",
    "\n",
    "'''\n",
    "\n",
    "Create the object of the Random Forest model\n",
    "You can also add other parameters and test your code here\n",
    "Some parameters are : n_estimators and max_depth\n",
    "Documentation of sklearn RandomForestClassifier: \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "'''\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# fit the model with the training data\n",
    "model.fit(train_x,train_y)\n",
    "\n",
    "# number of trees used\n",
    "print('Number of Trees used : ', model.n_estimators)\n",
    "\n",
    "# predict the target on the train dataset\n",
    "predict_train = model.predict(train_x)\n",
    "print('\\nTarget on train data',predict_train) \n",
    "\n",
    "# Accuray Score on train dataset\n",
    "accuracy_train = accuracy_score(train_y,predict_train)\n",
    "print('\\naccuracy_score on train dataset : ', accuracy_train)\n",
    "\n",
    "# predict the target on the test dataset\n",
    "predict_test = model.predict(test_x)\n",
    "print('\\nTarget on test data',predict_test) \n",
    "\n",
    "# Accuracy Score on test dataset\n",
    "accuracy_test = accuracy_score(test_y,predict_test)\n",
    "print('\\naccuracy_score on test dataset : ', accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Dimensionality Reduction Algorithms\n",
    "In the last 4-5 years, there has been an exponential increase in data capturing at every possible stages. Corporates/ Government Agencies/ Research organisations are not only coming with new sources but also they are capturing data in great detail.\n",
    "\n",
    "For example: E-commerce companies are capturing more details about customer like their demographics, web crawling history, what they like or dislike, purchase history, feedback and many others to give them personalized attention more than your nearest grocery shopkeeper.\n",
    "\n",
    "As a data scientist, the data we are offered also consist of many features, this sounds good for building good robust model but there is a challenge. How’d you identify highly significant variable(s) out 1000 or 2000? In such cases, dimensionality reduction algorithm helps us along with various other algorithms like Decision Tree, Random Forest, PCA, Factor Analysis, Identify based on correlation matrix, missing value ratio and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error  \n",
    "\n",
    "# read the train and test dataset\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# view the top 3 rows of the dataset\n",
    "print(train_data.head(3))\n",
    "\n",
    "# shape of the dataset\n",
    "print('\\nShape of training data :',train_data.shape)\n",
    "print('\\nShape of testing data :',test_data.shape)\n",
    "\n",
    "# Now, we need to predict the missing target variable in the test data\n",
    "# target variable - Survived\n",
    "\n",
    "# seperate the independent and target variable on training data\n",
    "# target variable - Item_Outlet_Sales\n",
    "train_x = train_data.drop(columns=['Item_Outlet_Sales'],axis=1)\n",
    "train_y = train_data['Item_Outlet_Sales']\n",
    "\n",
    "# seperate the independent and target variable on testing data\n",
    "test_x = test_data.drop(columns=['Item_Outlet_Sales'],axis=1)\n",
    "test_y = test_data['Item_Outlet_Sales']\n",
    "\n",
    "print('\\nTraining model with {} dimensions.'.format(train_x.shape[1]))\n",
    "\n",
    "# create object of model\n",
    "model = LinearRegression()\n",
    "\n",
    "# fit the model with the training data\n",
    "model.fit(train_x,train_y)\n",
    "\n",
    "# predict the target on the train dataset\n",
    "predict_train = model.predict(train_x)\n",
    "\n",
    "# Accuray Score on train dataset\n",
    "rmse_train = mean_squared_error(train_y,predict_train)**(0.5)\n",
    "print('\\nRMSE on train dataset : ', rmse_train)\n",
    "\n",
    "# predict the target on the test dataset\n",
    "predict_test = model.predict(test_x)\n",
    "\n",
    "# Accuracy Score on test dataset\n",
    "rmse_test = mean_squared_error(test_y,predict_test)**(0.5)\n",
    "print('\\nRMSE on test dataset : ', rmse_test)\n",
    "\n",
    "# create the object of the PCA (Principal Component Analysis) model\n",
    "# reduce the dimensions of the data to 12\n",
    "'''\n",
    "You can also add other parameters and test your code here\n",
    "Some parameters are : svd_solver, iterated_power\n",
    "Documentation of sklearn PCA:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "'''\n",
    "model_pca = PCA(n_components=12)\n",
    "\n",
    "new_train = model_pca.fit_transform(train_x)\n",
    "new_test  = model_pca.fit_transform(test_x)\n",
    "\n",
    "print('\\nTraining model with {} dimensions.'.format(new_train.shape[1]))\n",
    "\n",
    "# create object of model\n",
    "model_new = LinearRegression()\n",
    "\n",
    "# fit the model with the training data\n",
    "model_new.fit(new_train,train_y)\n",
    "\n",
    "# predict the target on the new train dataset\n",
    "predict_train_pca = model_new.predict(new_train)\n",
    "\n",
    "# Accuray Score on train dataset\n",
    "rmse_train_pca = mean_squared_error(train_y,predict_train_pca)**(0.5)\n",
    "print('\\nRMSE on new train dataset : ', rmse_train_pca)\n",
    "\n",
    "# predict the target on the new test dataset\n",
    "predict_test_pca = model_new.predict(new_test)\n",
    "\n",
    "# Accuracy Score on test dataset\n",
    "rmse_test_pca = mean_squared_error(test_y,predict_test_pca)**(0.5)\n",
    "print('\\nRMSE on new test dataset : ', rmse_test_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Gradient Boosting Algorithms\n",
    "10.1. GBM\n",
    "GBM is a boosting algorithm used when we deal with plenty of data to make a prediction with high prediction power. Boosting is actually an ensemble of learning algorithms which combines the prediction of several base estimators in order to improve robustness over a single estimator. It combines multiple weak or average predictors to a build strong predictor. These boosting algorithms always work well in data science competitions like Kaggle, AV Hackathon, CrowdAnalytix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# read the train and test dataset\n",
    "train_data = pd.read_csv('train-data.csv')\n",
    "test_data = pd.read_csv('test-data.csv')\n",
    "\n",
    "# shape of the dataset\n",
    "print('Shape of training data :',train_data.shape)\n",
    "print('Shape of testing data :',test_data.shape)\n",
    "\n",
    "# Now, we need to predict the missing target variable in the test data\n",
    "# target variable - Survived\n",
    "\n",
    "# seperate the independent and target variable on training data\n",
    "train_x = train_data.drop(columns=['Survived'],axis=1)\n",
    "train_y = train_data['Survived']\n",
    "\n",
    "# seperate the independent and target variable on testing data\n",
    "test_x = test_data.drop(columns=['Survived'],axis=1)\n",
    "test_y = test_data['Survived']\n",
    "\n",
    "'''\n",
    "Create the object of the GradientBoosting Classifier model\n",
    "You can also add other parameters and test your code here\n",
    "Some parameters are : learning_rate, n_estimators\n",
    "Documentation of sklearn GradientBoosting Classifier: \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n",
    "'''\n",
    "model = GradientBoostingClassifier(n_estimators=100,max_depth=5)\n",
    "\n",
    "# fit the model with the training data\n",
    "model.fit(train_x,train_y)\n",
    "\n",
    "# predict the target on the train dataset\n",
    "predict_train = model.predict(train_x)\n",
    "print('\\nTarget on train data',predict_train) \n",
    "\n",
    "# Accuray Score on train dataset\n",
    "accuracy_train = accuracy_score(train_y,predict_train)\n",
    "print('\\naccuracy_score on train dataset : ', accuracy_train)\n",
    "\n",
    "# predict the target on the test dataset\n",
    "predict_test = model.predict(test_x)\n",
    "print('\\nTarget on test data',predict_test) \n",
    "\n",
    "# Accuracy Score on test dataset\n",
    "accuracy_test = accuracy_score(test_y,predict_test)\n",
    "print('\\naccuracy_score on test dataset : ', accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.2. XGBoost\n",
    "Another classic gradient boosting algorithm that’s known to be the decisive choice between winning and losing in some Kaggle competitions.\n",
    "\n",
    "The XGBoost has an immensely high predictive power which makes it the best choice for accuracy in events as it possesses both linear model and the tree learning algorithm, making the algorithm almost 10x faster than existing gradient booster techniques.\n",
    "\n",
    "The support includes various objective functions, including regression, classification and ranking.\n",
    "\n",
    "One of the most interesting things about the XGBoost is that it is also called a regularized boosting technique. This helps to reduce overfit modelling and has a massive support for a range of languages such as Scala, Java, R, Python, Julia and C++.\n",
    "\n",
    "Supports distributed and widespread training on many machines that encompass GCE, AWS, Azure and Yarn clusters. XGBoost can also be integrated with Spark, Flink and other cloud dataflow systems with a built in cross validation at each iteration of the boosting process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# read the train and test dataset\n",
    "train_data = pd.read_csv('train-data.csv')\n",
    "test_data = pd.read_csv('test-data.csv')\n",
    "\n",
    "# shape of the dataset\n",
    "print('Shape of training data :',train_data.shape)\n",
    "print('Shape of testing data :',test_data.shape)\n",
    "\n",
    "# Now, we need to predict the missing target variable in the test data\n",
    "# target variable - Survived\n",
    "\n",
    "# seperate the independent and target variable on training data\n",
    "train_x = train_data.drop(columns=['Survived'],axis=1)\n",
    "train_y = train_data['Survived']\n",
    "\n",
    "# seperate the independent and target variable on testing data\n",
    "test_x = test_data.drop(columns=['Survived'],axis=1)\n",
    "test_y = test_data['Survived']\n",
    "\n",
    "'''\n",
    "Create the object of the XGBoost model\n",
    "You can also add other parameters and test your code here\n",
    "Some parameters are : max_depth and n_estimators\n",
    "Documentation of xgboost:\n",
    "\n",
    "https://xgboost.readthedocs.io/en/latest/\n",
    "'''\n",
    "model = XGBClassifier()\n",
    "\n",
    "# fit the model with the training data\n",
    "model.fit(train_x,train_y)\n",
    "\n",
    "\n",
    "# predict the target on the train dataset\n",
    "predict_train = model.predict(train_x)\n",
    "print('\\nTarget on train data',predict_train) \n",
    "\n",
    "# Accuray Score on train dataset\n",
    "accuracy_train = accuracy_score(train_y,predict_train)\n",
    "print('\\naccuracy_score on train dataset : ', accuracy_train)\n",
    "\n",
    "# predict the target on the test dataset\n",
    "predict_test = model.predict(test_x)\n",
    "print('\\nTarget on test data',predict_test) \n",
    "\n",
    "# Accuracy Score on test dataset\n",
    "accuracy_test = accuracy_score(test_y,predict_test)\n",
    "print('\\naccuracy_score on test dataset : ', accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.3. LightGBM\n",
    "LightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:\n",
    "\n",
    "Faster training speed and higher efficiency\n",
    "Lower memory usage\n",
    "Better accuracy\n",
    "Parallel and GPU learning supported\n",
    "Capable of handling large-scale data\n",
    "The framework is a fast and high-performance gradient boosting one based on decision tree algorithms, used for ranking, classification and many other machine learning tasks. It was developed under the Distributed Machine Learning Toolkit Project of Microsoft.\n",
    "\n",
    "Since the LightGBM is based on decision tree algorithms, it splits the tree leaf wise with the best fit whereas other boosting algorithms split the tree depth wise or level wise rather than leaf-wise. So when growing on the same leaf in Light GBM, the leaf-wise algorithm can reduce more loss than the level-wise algorithm and hence results in much better accuracy which can rarely be achieved by any of the existing boosting algorithms.\n",
    "\n",
    "Also, it is surprisingly very fast, hence the word ‘Light’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.rand(500, 10) # 500 entities, each contains 10 features\n",
    "label = np.random.randint(2, size=500) # binary target\n",
    "\n",
    "train_data = lgb.Dataset(data, label=label)\n",
    "test_data = train_data.create_valid('test.svm')\n",
    "\n",
    "param = {'num_leaves':31, 'num_trees':100, 'objective':'binary'}\n",
    "param['metric'] = 'auc'\n",
    "\n",
    "num_round = 10\n",
    "bst = lgb.train(param, train_data, num_round, valid_sets=[test_data])\n",
    "\n",
    "bst.save_model('model.txt')\n",
    "\n",
    "# 7 entities, each contains 10 features\n",
    "data = np.random.rand(7, 10)\n",
    "ypred = bst.predict(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.4. Catboost\n",
    "CatBoost is a recently open-sourced machine learning algorithm from Yandex. It can easily integrate with deep learning frameworks like Google’s TensorFlow and Apple’s Core ML.\n",
    "\n",
    "The best part about CatBoost is that it does not require extensive data training like other ML models, and can work on a variety of data formats; not undermining how robust it can be.\n",
    "\n",
    "Make sure you handle missing data well before you proceed with the implementation.\n",
    "\n",
    "Catboost can automatically deal with categorical variables without showing the type conversion error, which helps you to focus on tuning your model better rather than sorting out trivial errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "#Read training and testing files\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "#Imputing missing values for both train and test\n",
    "train.fillna(-999, inplace=True)\n",
    "test.fillna(-999,inplace=True)\n",
    "\n",
    "#Creating a training set for modeling and validation set to check model performance\n",
    "X = train.drop(['Item_Outlet_Sales'], axis=1)\n",
    "y = train.Item_Outlet_Sales\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, train_size=0.7, random_state=1234)\n",
    "categorical_features_indices = np.where(X.dtypes != np.float)[0]\n",
    "\n",
    "#importing library and building model\n",
    "from catboost import CatBoostRegressormodel=CatBoostRegressor(iterations=50, depth=3, learning_rate=0.1, loss_function='RMSE')\n",
    "\n",
    "model.fit(X_train, y_train,cat_features=categorical_features_indices,eval_set=(X_validation, y_validation),plot=True)\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "\n",
    "submission['Item_Identifier'] = test['Item_Identifier']\n",
    "submission['Outlet_Identifier'] = test['Outlet_Identifier']\n",
    "submission['Item_Outlet_Sales'] = model.predict(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
